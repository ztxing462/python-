{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 8.9082e-39, 1.8754e+28],\n",
      "        [4.0611e-08, 4.2420e-08, 1.2869e-11]])\n",
      "tensor([[0.5371, 0.8896, 0.7393],\n",
      "        [0.9494, 0.7148, 0.0201]])\n",
      "tensor([[7, 7, 7],\n",
      "        [7, 7, 7]])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a_t = torch.empty(2,3)\n",
    "print(a_t)\n",
    "a_t = torch.rand(2,3)\n",
    "print(a_t)\n",
    "a_t = torch.full([2,3],7)\n",
    "print(a_t)\n",
    "a_t = torch.eye(3,3)\n",
    "print(a_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float32\n",
      "torch.float64\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a_t = torch.tensor([1.2,2.1])\n",
    "print(a_t.dtype)\n",
    "a_t = torch.FloatTensor([1.2,2.1])\n",
    "print(a_t.dtype) #尝试 torch.IntTensor 声明 int 型 tensor\n",
    "a_t = a_t.double() #尝试 a_t.float() 及 a_t.int() 实现数据类型转换\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "a_t = torch.tensor([1.2,2.1])\n",
    "print(a_t.dtype)\n",
    "b_t = torch.FloatTensor([1.2,2.1])\n",
    "print(b_t.dtype)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 3); b = torch.ones(3)\n",
    "print(a+b) #广播\n",
    "print(torch.add(a, b))\n",
    "print(a.add(b)); print(a)\n",
    "print(a.add_(b)); print(a) #函数名以 结尾的是 inplace 方式，会修改调用者自己的数据"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([])\n",
      "torch.Size([3])\n",
      "torch.Size([2])\n",
      "torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 3);\n",
    "print(a.shape)\n",
    "print(torch.sum(a).shape)\n",
    "print(a.sum(axis=0).shape) #压缩指定维度\n",
    "print(a.sum(axis=1).shape)\n",
    "print(a.sum(axis=1, keepdim=True).shape) #保留维度"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "torch.Size([2, 12])\n",
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a=torch.ones(2,3,4)\n",
    "print(a.size())\n",
    "b=torch.flatten(a, start_dim=1)\n",
    "print(b.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([])\n",
      "torch.Size([3])\n",
      "torch.Size([2])\n",
      "torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.ones(2, 3);\n",
    "print(a.shape)\n",
    "print(torch.sum(a).shape)\n",
    "print(a.sum(axis=0).shape) #压缩指定维度\n",
    "print(a.sum(axis=1).shape)\n",
    "print(a.sum(axis=1, keepdim=True).shape) #保留维度"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "torch.Size([3, 1, 2])\n",
      "torch.Size([3, 2])\n",
      "torch.Size([3, 1, 2, 1])\n",
      "torch.Size([3, 1, 2])\n",
      "torch.Size([1, 3, 2])\n",
      "torch.Size([1, 3, 2])\n",
      "tensor([[[-0.5911],\n",
      "         [ 0.4556]],\n",
      "\n",
      "        [[-0.3899],\n",
      "         [-0.6758]]])\n",
      "tensor([[[-0.5911, -0.5911, -0.5911],\n",
      "         [ 0.4556,  0.4556,  0.4556]],\n",
      "\n",
      "        [[-0.3899, -0.3899, -0.3899],\n",
      "         [-0.6758, -0.6758, -0.6758]]])\n",
      "tensor([[[-0.5911, -0.5911, -0.5911],\n",
      "         [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.3899, -0.3899, -0.3899],\n",
      "         [-0.6758, -0.6758, -0.6758]]])\n"
     ]
    }
   ],
   "source": [
    "# view/reshape\n",
    "#保证 tensor 数据大小不变，对 tensor 数据进行形状的重新定义与转换\n",
    "import torch\n",
    "a_t= torch.tensor ([1, 2, 3, 4, 5, 6]); b_t = a_t.view (3,2)\n",
    "print(b_t)\n",
    "# squeeze/unsqueeze 删减维度或者增加维度操作\n",
    "a_t= torch.rand (3, 1, 2); print(a_t.shape)\n",
    "b_t= a_t.squeeze(); print(b_t.shape); c_t = a_t.unsqueeze (3);print( c_t.shape)\n",
    "# transpose/permute：类似矩阵转置操作，对多维的数据具有多次或者单次的转换操作\n",
    "\n",
    "a_t= torch.rand (3, 1, 2); print(a_t.shape); b_t = a_t.transpose (0,1); print(b_t.shape)\n",
    "c_t= a_t.permute (1,0,2);\n",
    "print(c_t.shape)\n",
    "# expand：扩展某个 size 为 1 的维度。如 ( 扩展为 (\n",
    "\n",
    "x=torch.randn(2,2,1); print(x); y=x.expand(2,2,3); print(y); x[0,1] = 0; print(y) #共享内存"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0323)\n",
      "torch.Size([2, 1, 28, 28])\n",
      "torch.Size([2, 2, 28, 28])\n",
      "torch.Size([2, 1, 28, 28])\n",
      "torch.Size([4, 3, 14, 14])\n",
      "torch.Size([4, 3, 14, 14])\n",
      "torch.Size([4, 28, 28])\n",
      "torch.Size([4, 3, 28, 2])\n",
      "torch.Size([3, 28, 14])\n"
     ]
    }
   ],
   "source": [
    "#当遇到 Tensor 不支持的操作时，可先转成 Numpy 数组，处理后再转回 tensor ，开销很小。\n",
    "import torch\n",
    "a =torch.rand (4, 3, 28,28)\n",
    "print(a[0, 0, 2, 4]) #具体到某个元素\n",
    "print(a[:2, :1, :, :].shape) #在第一个维度上取 0 和 1, 在第二个维度上取 0\n",
    "print(a[:2, 1:, :, :].shape) #在第一个维度上取 0 和 1, 在第二个维度上取 1,2\n",
    "print(a[:2,2:, :, :].shape) # 在第一个维度上取 0 和 1, 在第二个维度上取 1,2\n",
    "print(a[:, :, 0:28:2, 0:28:2].shape) # step=2隔行采样\n",
    "print(a[:, :, ::2, ::2].shape) #等同于这个\n",
    "print(a[:, 1, ...].shape)\n",
    "print(a[..., :2].shape)\n",
    "print(a[0, ..., ::2].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}